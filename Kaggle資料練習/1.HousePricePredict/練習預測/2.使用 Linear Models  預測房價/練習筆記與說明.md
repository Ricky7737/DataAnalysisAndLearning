# 使用 Linear Models 預測房價 程式陌生語法 與 "回歸模型"學習 筆記
* 程式碼 : https://github.com/Ricky7737/DataAnalysisAndLearning/blob/main/Kaggle%E8%B3%87%E6%96%99%E7%B7%B4%E7%BF%92/1.HousePricePredict/%E7%B7%B4%E7%BF%92%E9%A0%90%E6%B8%AC/2.%E4%BD%BF%E7%94%A8%20Linear%20Models%20%20%E9%A0%90%E6%B8%AC%E6%88%BF%E5%83%B9/PredictUseLinearModels.ipynb
## 1.取對數 np.log1p
  * 可以將資料分布從偏態分布取 log 後變成正態分布
  * np.log1p 為 取對數後 +1，用意在於 x = 0 時變成 log(0)，這個在數學上是未被定義的，而 log(1) 則等於 0，所以 x 都要加上 1。因為我們原始的資料中，部分欄位的是 0，所以必須 +1 才不會使資料出現錯誤
  * 另外 np.log1p 可以對較大的數值進行壓縮，同時拉伸較小的數值，使資料分布更加接近 **正態分布**
  * 以下為語法 :
 ```
    np.log1p(data['要取對數的目標欄位'])
 ```

## 2. Sklearn Liner Regression(線性迴歸) (L1 Lasso) 與 (L2 Ridge)
 * 資料來源引用 1 : https://roger010620.medium.com/linear-regression-lasso-ridge%E4%BB%8B%E7%B4%B9-%E9%99%84python-code-76e89af4dce9
 * 資料來源引用 1 : https://ithelp.ithome.com.tw/articles/10227654
 * 這三組方式同樣都是先篩選出一個最佳的 alpha 與"分配特徵權重"來預測目標資料(Y)，而 "Ridge" 與 "Lasso" 正則化相比"單純線性回歸"而言更加重在於對權重的懲罰，而這可以降低模型過度擬合的風險
  ### 2.1 Lasso
   * Lasso 為 Linear Regression 加上 L1
   * 而 Lasso 會使用正規化來優化模型，而且可以在訓練模型中剃除不重要的變數(特徵)
   * Lasso Python 語法架構
```
    #套件安裝
    pip install scikit-learn
    
    # 載入 Lasso 套件
    from sklearn.liner_model import Lasso
    reg = Lasso(alpha= 自己設定一個數值)  # alpha 可以調整，來強化正則化強度
    lasso_model.fit(X_train, y_train)   # 使用 Lasso 訓練數據

    # 進行預測
    y_pred =  lasso_model.fit(X_train, y_train)

    # 查看特徵權重
    coefficients = lasso_model.coef_
```

  ### 2.2 Ridge (L2 正則化)
   * Ridge 為 Linear Regression 加上 L2
   * Ridge 在分配模型權重時，會將較於無關的特徵做懲罰，而懲罰越大時就會迫使權重接近 0 ，但相較於 L1 Lasso 則還是會保留特徵。
   * 而我參考練習得程式碼，他則預先列出 alpha 值，並繪製 RMSE(均方根誤差, Root Mean Square Error) 曲線圖來挑出最適合的 alpha 值 (詳細程式碼請參考程式碼)
   * Ridge 基本語法架構
```
# 載入 Ridge 套件
from sklearn.linear_model import Ridge
# 調整 alpha 來強化 L2 正則化強度
ridge_model = Ridge(alpha= 放入適合的數值)
# 訓練模型
ridge_model.fit(訓練資料 X_train, 訓練的預測值 y_train)
# 預測
y_pred = ridge_model.predict(X_test)
# 評估模型性能
rmse = np.sqrt(mean_sqared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)
```

  ### 2.3 Lasso 與 Ridge 正則化 比較 與 適用數據
   #### 2.3.1 Lasso (L1 正則化)
   * 如前面所提到，因為 Lasso 會替除掉無關的特徵，可以減少模型訓練的複雜度，所以可能會比較適用於多維度的資料
   * 例如:本練習所使用的房地產售價資料，因為裡面有多種特徵，像是時間、房屋材質、土地大小、臥室浴室有幾間之類的。
     
   #### 2.3.2 Ridge
   * 如前面提到的，由於 Ridge 不會剔除較無相關的特徵，只會減小他們對模型的影響力，藉由平衡模型與降低過度擬合 
   * 而 Ridge 可以解決多重共線性的問題，多重共線性亦就是資料中存在多個變數之間高度相關的關係，這會使回歸估計正確率降低
   * 所以 Ridge 可能會比較適合用在像是金融資料
##### 最後重點，使用 Lasso 與 Ridge，可以透過去挑選出適合的 alpha 值，去計算出 RMSE 後再來比較那個 RMSE 比較低，進而選擇適合的方法來訓練資料

   ### 2.4 額外補充解讀 RMSE 與 R-squared 的方式
   #### 2.4.1 RMSE(均方根誤差，Mean Squared Eror)
   * 1.RMSE 越接近零當然是越好的，也就是說"預測的結果"與"實際結果"非常接近.
   * 2.而 RMSE 數值也比需參考因變數的單位，如預測房價時使用千美元，RMSE結果會與原數據單位相同
   ##### RMSE 適用時機 
   * 適用於各種回歸預測模型，例如房價、股票等等
   * RMSE 對於離群值比較敏感，所以可能在數據前清理時可以考慮清除掉少量離群值來再來計算 RMSE
   * 用於比較不同方式回歸模型的性能，並選擇最佳模型
   #### 2.4.2 R-squared 又稱為判定係數(coefficient of determination)
   * R-squared 數值範圍取值在 0 ≤ R² ≤ 1
   * R² = 0 時表示無法解釋目標變數之變異性，模型無法擬合數據
   * 但 R² = 1 則表示模型完全擬合數據，也就是預測結果非常貼近真實數據。但實際上很難大於 1 以上，因為實際的模型存在著噪聲、測量誤差等問題。如果擬合程度非常高，當模型在未來預測新數據的時候，可能無法泛化到新的數據
   * 若 R² 為負值，表示模型擬合程度非常差，可能存在問題需要解決
   * 通常 R² > 0.75 時 : 表示回歸模型的擬合度良好，解釋能力程度較高
   * 0.5 < R² < 0.75 時 : 雖說模型擬合度尚可，但仍可以進一步調整修正模型來提升解釋能力與準確度
   * 而 R² < 0.5 則表示擬合有問題，需要重新篩選訓練的自變數來提升擬合度
     
## 3.XGBoost （eXtreme Gradient Boosting）
參考網址 : https://ithelp.ithome.com.tw/articles/10273094
###### XGBoost 採用隨機採樣的方式與隨機森林方法一樣生成多棵樹，而這會使模型可能過於複雜化，而噪音也會隨之增加影響模型判斷能力，所以 XGBoost 結合正則化使損失函數更加平化
